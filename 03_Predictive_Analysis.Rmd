---
title: "03_Predictive_Analysis"
output: html_document
date: "2024-10-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidymodels)
library(ranger)
library(randomForestSRC)
library(randomForest)
library(data.table)
library(xgboost)
library(vip)
library(rpart)
library(MLmetrics)
library(doParallel)
```


## Preparing the Data for Predictive Modelling

Before predictive modelling, the tidy data will undergo further process to ensure it meets the modelling requirement. Such further processing are treating data outliers and missing value. Outliers and missing value for each element have been identified in exploratory data analysis, however only elements from test ME-4ACD81 that will be treated, as these are the elements that will be used in predicting the REE, HREE, and LREE concentrations value.

The process starts from loading the tidy data and convert it from long table to wide table.
```{r}
# Load data
alldata <- readRDS('./results/all_data.rds')

DT <- setDT(alldata)

# Reformat data into wide format
DT <- dcast(DT,Project_Name+Sample_ID~Element_Symbol, value.var = c("Element_Value_ppm"), fun=mean, fill=NA)
```

The code below creates a function to replace outliers values. The function will replace outliers value with the median value of each element from their respective project area.  
```{r}
# Function to replace outliers directly and create a log of replaced values
replace_outliers_and_log <- function(data, project_var, target_vars) {
  # Create an empty data frame to track replaced values
  outlier_log <- data.frame(Project_Name = character(),
                            Element = character(),
                            Original_Value = numeric(),
                            Replaced_Value = numeric(),
                            stringsAsFactors = FALSE)

  # Loop through each target variable and apply the replacement logic
  for (target_var in target_vars) {
    # Calculate the median of the target variable for each Project_Name
    medians <- data %>%
      group_by({{ project_var }}) %>%
      summarize(project_median = median(.data[[target_var]], na.rm = TRUE)) %>%
      ungroup()
    
    # Join the median values back to the original data
    data <- data %>%
      left_join(medians, by = rlang::as_name(enquo(project_var))) %>%
      rename(!!paste0("median_", target_var) := project_median)  # Rename to unique median column for each variable
    
    # Calculate IQR and identify outliers for the target variable globally
    Q1 <- quantile(data[[target_var]], 0.25, na.rm = TRUE)
    Q3 <- quantile(data[[target_var]], 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    lower_bound <- Q1 - 1.5 * IQR
    upper_bound <- Q3 + 1.5 * IQR
    
    # Replace outliers directly in the target variable and log replaced values
    data <- data %>%
      mutate(
        # Replace outliers with the median value specific to the Project_Name
        replaced_value = ifelse(.data[[target_var]] < lower_bound | .data[[target_var]] > upper_bound, .data[[paste0("median_", target_var)]], NA),
        # Log the original value and the replaced value in the log dataframe
        outlier_flag = !is.na(replaced_value)
      )
    
    # Create a log of replaced values
    temp_log <- data %>%
      filter(outlier_flag) %>%
      transmute(
        Project_Name = .data[[rlang::as_name(enquo(project_var))]],
        Element = target_var,
        Original_Value = .data[[target_var]],
        Replaced_Value = replaced_value
      )
    
    # Combine with the main log dataset
    outlier_log <- bind_rows(outlier_log, temp_log)
    
    # Replace the actual outliers in the original data
    data <- data %>%
      mutate(!!target_var := ifelse(outlier_flag, replaced_value, .data[[target_var]])) %>%
      # Remove temporary columns
      select(-paste0("median_", target_var), -replaced_value, -outlier_flag)
  }
  
  return(list(cleaned_data = data, outlier_log = outlier_log))
}

# Example usage for multiple elements (e.g., Pb, Ag, Au) with Project_Name as the reference grouping variable
result <- replace_outliers_and_log(DT, Project_Name, c("Ag", "Cd", "Co", "Cu", "Li", "Mo", "Ni", "Pb", "Sc", "Tl", "Zn"))

# Extract the cleaned data and the log of replaced values
DT <- result$cleaned_data
outlier_log <- result$outlier_log

```


The code below will treat the missing value. Similar to the outliers treatment, in this case missing value will be replaced by the median value of each element from their respective project area. However, in the case that the project does not contain the missing element, then global median value will be used. 
```{r}

# Calculate global medians for all the target variables before grouping
global_medians <- DT %>%
  summarize(
    across(
      c(Ag, Cd, Co, Cu, Li, Mo, Ni, Pb, Sc, Tl, Zn),  # List of elements/columns
      ~ median(., na.rm = TRUE),
      .names = "global_{.col}"  # Store global medians with unique names (e.g., global_Ag)
    )
  )

# Replace missing values with Project_Name-specific medians or global median if none exist
DT_alt <- DT %>%
  filter(!is.na(REE)) %>%
  # Calculate Project_Name-specific medians for each element
  group_by(Project_Name) %>%
  mutate(
    across(
      .cols = c(Ag, Cd, Co, Cu, Li, Mo, Ni, Pb, Sc, Tl, Zn),  # Specify target elements
      .fns = ~ ifelse(
        # If the value is missing, replace it
        is.na(.),
        # Check if all values in the group are missing for this element
        ifelse(
          all(is.na(.)), 
          # If all are missing, use the global median (stored in `global_medians`)
          global_medians[[paste0("global_", cur_column())]],
          # Otherwise, use the Project_Name-specific median
          median(., na.rm = TRUE)
        ), 
        .
      )
    )
  ) %>%
  ungroup()

```



### Split data into train and test data sets
Finally, before building the model, the data will be splitted into training and test set by 2/3 proportion and by considering the project area.
```{r}
set.seed(2022)
DT_initial <- initial_split(DT_alt, prop = 2/3, strata = Project_Name)
DT_train <- training(DT_initial)
DT_test <- testing(DT_initial)
```


## Predictive Modelling (REE)
The predictive modelling will start from predicting the elements from test ME-4ACD81 to REE. HREE and LREE will have separate section.

### Random Forest Model: Option 1 (default model with all elements)

Combination of all elements from test ME-4ACD81 in predicting REE.

#### Data Preparation
```{r}
# Filter the training and test datasets to include predictors and target variables only
DT_train_fltr_alt <- DT_train[,c("REE","Ag", "Cd", "Co", "Cu", "Li", "Mo", "Ni", "Pb", "Sc", "Tl", "Zn")] 
DT_test_fltr_alt <- DT_test[,c("REE","Ag", "Cd", "Co", "Cu", "Li", "Mo", "Ni", "Pb", "Sc", "Tl", "Zn")]

```

#### Build the Model
```{r}
# Build RF Model
rf_spec <- rand_forest(mode = "regression") %>%
  set_engine("randomForest")

set.seed(2002)
rf_fit <- rf_spec %>%
  fit(REE ~ ., data = DT_train_fltr_alt)

# Make predictions on the preprocessed testing data
rf_tr <- predict(rf_fit, new_data = DT_train_fltr_alt)
rf_ts <- predict(rf_fit, new_data = DT_test_fltr_alt)
```

#### Evaluate the Model
```{r}

# RF Default Results
## Training set evaluation
results_train_rf <- rf_tr %>%
  bind_cols(DT_train_fltr_alt) %>%
  select(.pred, REE) %>%
  rename(truth = REE) %>%
  mutate(model = "RF_Default")

## Test set evaluation
results_test_rf <- rf_ts %>%
  bind_cols(DT_test_fltr_alt) %>%
  select(.pred, REE) %>%
  rename(truth = REE) %>%
  mutate(model = "RF_Default")

# Show the evaluation metrics
## Training set
results_train_rf %>%
  group_by(model) %>%
  summarise(RMSE = RMSE(.pred, truth),
            R_Square = R2_Score(.pred, truth),
            MAE = MAE(.pred, truth),
            MAPE = MAPE(.pred, truth))

## Test set
results_test_rf %>%
  group_by(model) %>%
  summarise(RMSE = RMSE(.pred, truth),
            R_Square = R2_Score(.pred, truth),
            MAE = MAE(.pred, truth),
            MAPE = MAPE(.pred, truth))
```

#### Prediction vs Actual Comparison
```{r viz_comparison_rf, warning = FALSE, error = FALSE}
results_test_rf %>%
  mutate(train = "testing") %>%
  bind_rows(results_train_rf %>%
              mutate(train = "training")) %>%
  ggplot(aes(.pred, truth, , color = model)) +
  geom_abline(lty = 2, size = 0.75, linetype = "dashed") +
  geom_point(alpha = 0.7) +
  facet_wrap(~train)
```

### Random Forest Model: Option 2 (Pb, Sc)
Combination of Pb, Sc, Zn in predicting REE. These two elements have moderately strong correlation with REE where their correlation coeff. are above 0,5. Starting from this, hyperparameter tuning will be performed.

#### Data Preparation
```{r}
# Filter the training and test datasets to include predictors and target variables only
DT_train_fltr_alt_2 <- DT_train[,c("REE","Pb","Sc")]
DT_test_fltr_alt_2 <- DT_test[,c("REE","Pb","Sc")]

# Create a cross-validation data for resampling purposes during hyperparameter tuning
set.seed(2022)
DT_fltr_folds_2 <- vfold_cv(DT_train_fltr_alt_2, v = 5, repeats = 2)

```

#### Build the Model
```{r}
# Build RF Tuned Model
rf_tuned_spec <- rand_forest(mode = "regression",
                       mtry = tune(),
                       min_n = tune(),
                       trees = tune()) %>%
                 set_engine("randomForest")

# Create a recipe for preprocessing
rf_recipe_2 <- recipe(REE ~ ., data = DT_train_fltr_alt_2)

# Create a workflow
rf_tuned_workflow_2 <- workflow() %>%
  add_recipe(rf_recipe_2) %>%
  add_model(rf_tuned_spec)

# Set-up the hyperparameter tuning
rf_tuned_grid_2 <- grid_regular(
  mtry(range = c(1,2)),
  min_n(range = c(1,15)),
  trees(range = c(500,2000)),
  levels = 5)

# Perform the hyperparameter tuning
set.seed(2022)
rf_random_tune_2 <- tune_grid(
  rf_tuned_workflow_2,
  resamples = DT_fltr_folds_2,
  grid = rf_tuned_grid_2,
  metrics = metric_set(rmse, rsq, mae)
)

# Show the best configurations
show_best(rf_random_tune_2, n = 3)

# Fit the best model into the training set
set.seed(2022)
rf_random_tuned_fit_2 <- finalize_workflow(rf_tuned_workflow_2, select_best(rf_random_tune_2)) %>%
    fit(DT_train_fltr_alt_2)

# Apply the trained recipe to the testing data
# 'bake()' will apply all recipe steps (e.g., imputation, normalization) to the testing dataset
DT_train_fltr_alt_2_prep <- bake(prep(rf_recipe_2), new_data = DT_train_fltr_alt_2)
DT_test_fltr_alt_2_prep <- bake(prep(rf_recipe_2), new_data = DT_test_fltr_alt_2)

# Make predictions on the preprocessed testing data
rf_tr_rdm_tuned_2_pred <- predict(rf_random_tuned_fit_2, new_data = DT_train_fltr_alt_2_prep)
rf_ts_rdm_tuned_2_pred <- predict(rf_random_tuned_fit_2, new_data = DT_test_fltr_alt_2_prep)
```

#### Evaluate the Model
```{r}
# RF Tuned Results for Pb & Sc
## Training set evaluation
results_train_rf_rdm_tuned_2 <- rf_tr_rdm_tuned_2_pred %>%
  bind_cols(DT_train_fltr_alt_2) %>%
  select(.pred, REE) %>%
  rename(truth = REE) %>%
  mutate(model = "RF_Rdm_Tuned_2")

## Test set evaluation
results_test_rf_rdm_tuned_2 <- rf_ts_rdm_tuned_2_pred %>%
  bind_cols(DT_test_fltr_alt_2) %>%
  select(.pred, REE) %>%
  rename(truth = REE) %>%
  mutate(model = "RF_Rdm_Tuned_2")

# Merge the results
## Training set
results_train <- bind_rows(results_train_rf, results_train_rf_rdm_tuned_2)

## Test set
results_test <- bind_rows(results_test_rf, results_test_rf_rdm_tuned_2)

# Show the evaluation metrics for every model
## Training set
results_train %>%
  group_by(model) %>%
  summarise(RMSE = RMSE(.pred, truth),
            R_Square = R2_Score(.pred, truth),
            MAE = MAE(.pred, truth),
            MAPE = MAPE(.pred, truth))

## Test set
results_test %>%
  group_by(model) %>%
  summarise(RMSE = RMSE(.pred, truth),
            R_Square = R2_Score(.pred, truth),
            MAE = MAE(.pred, truth),
            MAPE = MAPE(.pred, truth))
```

#### Prediction vs Actual Comparison 
```{r viz_comparison2, warning = FALSE, error = FALSE}
results_test %>%
  mutate(train = "testing") %>%
  bind_rows(results_train %>%
              mutate(train = "training")) %>%
  ggplot(aes(truth, .pred, color = model)) +
  geom_abline(lty = 2, size = 0.75, linetype = "dashed") +
  geom_point(alpha = 0.7) +
  facet_wrap(~train)
```


### Random Forest Model: Option 3 (All elements from ME-4ACD81)
Combination of All elements from ME-4ACD81 in predicting REE.

#### Data Preparation
```{r}
# Filter the training and test datasets to include predictors and target variables only
DT_train_fltr_alt_3 <- DT_train[,c("REE","Ag", "Cd", "Co", "Cu", "Li", "Mo", "Ni", "Pb", "Sc", "Tl", "Zn")] 
DT_test_fltr_alt_3 <- DT_test[,c("REE","Ag", "Cd", "Co", "Cu", "Li", "Mo", "Ni", "Pb", "Sc", "Tl", "Zn")]

# Create a cross-validation data for resampling purposes during hyperparameter tuning
set.seed(2022)
DT_fltr_folds_3 <- vfold_cv(DT_train_fltr_alt_3, v = 5, repeats = 2)

```

#### Build the Model
```{r}
# Create a recipe for preprocessing
rf_recipe_3 <- recipe(REE ~ ., data = DT_train_fltr_alt_3)

# Create a workflow
rf_tuned_workflow_3 <- workflow() %>%
  add_recipe(rf_recipe_3) %>%
  add_model(rf_tuned_spec)

# Set-up the hyperparameter tuning
rf_tuned_grid_3 <- grid_regular(
  mtry(range = c(1,11)),
  min_n(),
  trees(),
  levels = 5)

# Perform the hyperparameter tuning search
set.seed(2022)
rf_random_tune_3 <- tune_grid(
  rf_tuned_workflow_3,
  resamples = DT_fltr_folds_3,
  grid = rf_tuned_grid_3,
  metrics = metric_set(rmse, rsq, mae)
)

# Show the best configurations
show_best(rf_random_tune_3, n = 3)

# Fit the best model into the training set
set.seed(2022)
rf_random_tuned_fit_3 <- finalize_workflow(rf_tuned_workflow_3, select_best(rf_random_tune_3)) %>%
    fit(DT_train_fltr_alt_3)

# Apply the trained recipe to the testing data
# 'bake()' will apply all recipe steps (e.g., imputation, normalization) to the testing dataset
DT_train_fltr_alt_3_prep <- bake(prep(rf_recipe_3), new_data = DT_train_fltr_alt_3)
DT_test_fltr_alt_3_prep <- bake(prep(rf_recipe_3), new_data = DT_test_fltr_alt_3)

# Make predictions on the preprocessed testing data
rf_tr_rdm_tuned_3_pred <- predict(rf_random_tuned_fit_3, new_data = DT_train_fltr_alt_3_prep)
rf_ts_rdm_tuned_3_pred <- predict(rf_random_tuned_fit_3, new_data = DT_test_fltr_alt_3_prep)
```

```{r}
# Plot variable importance
rf_random_tuned_fit_3 %>%
  extract_fit_parsnip() %>%
  vip::vip()
```


#### Evaluate the Model
```{r}
# RF Tuned Results for all elements from ME-4ACD81
## Training set evaluation
results_train_rf_rdm_tuned_3 <- rf_tr_rdm_tuned_3_pred %>%
  bind_cols(DT_train_fltr_alt_3) %>%
  select(.pred, REE) %>%
  rename(truth = REE) %>%
  mutate(model = "RF_Rdm_Tuned_3")

## Test set evaluation
results_test_rf_rdm_tuned_3 <- rf_ts_rdm_tuned_3_pred %>%
 bind_cols(DT_test_fltr_alt_3) %>%
  select(.pred, REE) %>%
  rename(truth = REE) %>%
  mutate(model = "RF_Rdm_Tuned_3")

# Merge the results
## Training set
results_train <- bind_rows(results_train, results_train_rf_rdm_tuned_3)

## Test set
results_test <- bind_rows(results_test, results_test_rf_rdm_tuned_3)

# Show the evaluation metrics for every model
## Training set
results_train %>%
  group_by(model) %>%
  summarise(RMSE = RMSE(.pred, truth),
            R_Square = R2_Score(.pred, truth),
            MAE = MAE(.pred, truth),
            MAPE = MAPE(.pred, truth))

## Test set
results_test %>%
  group_by(model) %>%
  summarise(RMSE = RMSE(.pred, truth),
            R_Square = R2_Score(.pred, truth),
            MAE = MAE(.pred, truth),
            MAPE = MAPE(.pred, truth))
```

#### Prediction vs Actual Comparison
```{r viz_comparison3, warning = FALSE, error = FALSE}
results_test %>%
  mutate(train = "testing") %>%
  bind_rows(results_train %>%
              mutate(train = "training")) %>%
  ggplot(aes(.pred, truth, color = model)) +
  geom_abline(lty = 2, size = 0.75, linetype = "dashed") +
  geom_point(alpha = 0.7) +
  facet_wrap(~train)
```

### Random Forest Model: Option 4 (Pb, Tl, Sc, Zn, Cu, Cd)
Combination of elements Pb, Tl, Sc, Zn, Cu, Cd in predicting REE. These six elements have high variable importance when creating the model with all elements included (Option 3).

#### Data Preparation
```{r}
# Filter the training and test datasets to include predictors and target variables only
DT_train_fltr_alt_4 <- DT_train[,c("REE","Pb","Tl","Sc","Zn", "Cu", "Cd")] 
DT_test_fltr_alt_4 <- DT_test[,c("REE","Pb","Tl","Sc","Zn", "Cu", "Cd")] 

# Create a cross-validation table for resampling purposes during hyperparameter tuning
set.seed(2022)
DT_fltr_folds_4 <- vfold_cv(DT_train_fltr_alt_4, v = 5, repeats = 2)

```

#### Build the Model
```{r}
# Create a recipe for preprocessing
rf_recipe_4 <- recipe(REE ~ ., data = DT_train_fltr_alt_4)
   
# Create a workflow
rf_tuned_workflow_4 <- workflow() %>%
  add_recipe(rf_recipe_4) %>%
  add_model(rf_tuned_spec)

# Set-up the hyperparameter tuning
rf_tuned_grid_4 <- grid_regular(
  mtry(range = c(1,6)),
  min_n(),
  trees(),
  levels = 5)

# Perform the hyperparameter tuning
set.seed(2022)
rf_random_tune_4 <- tune_grid(
  rf_tuned_workflow_4,
  resamples = DT_fltr_folds_4,
  grid = rf_tuned_grid_4,
  metrics = metric_set(rmse, rsq, mae)
)

# Show the best configurations
show_best(rf_random_tune_4, n = 3)

# Fit the best model into the training set
set.seed(2022)
rf_random_tuned_fit_4 <- finalize_workflow(rf_tuned_workflow_4, select_best(rf_random_tune_4)) %>%
    fit(DT_train_fltr_alt_4)

# Apply the trained recipe to the testing data
# 'bake()' will apply all recipe steps (e.g., imputation, normalization) to the testing dataset
DT_train_fltr_alt_4_prep <- bake(prep(rf_recipe_4), new_data = DT_train_fltr_alt_4)
DT_test_fltr_alt_4_prep <- bake(prep(rf_recipe_4), new_data = DT_test_fltr_alt_4)

# Make predictions on the preprocessed testing data
rf_tr_rdm_tuned_4_pred <- predict(rf_random_tuned_fit_4, new_data = DT_train_fltr_alt_4_prep)
rf_ts_rdm_tuned_4_pred <- predict(rf_random_tuned_fit_4, new_data = DT_test_fltr_alt_4_prep)
```

#### Evaluate the Model
```{r}
# RF Tuned Results
## Training set evaluation
results_train_rf_rdm_tuned_4 <- rf_tr_rdm_tuned_4_pred %>%
  bind_cols(DT_train_fltr_alt_4) %>%
  select(.pred, REE) %>%
  rename(truth = REE) %>%
  mutate(model = "RF_Rdm_Tuned_4")

## Test set evaluation
results_test_rf_rdm_tuned_4 <- rf_ts_rdm_tuned_4_pred %>%
  bind_cols(DT_test_fltr_alt_4) %>%
  select(.pred, REE) %>%
  rename(truth = REE) %>%
  mutate(model = "RF_Rdm_Tuned_4")


# Merge the results
## Training set
results_train <- bind_rows(results_train, results_train_rf_rdm_tuned_4)

## Test set
results_test <- bind_rows(results_test, results_test_rf_rdm_tuned_4)

# Show the evaluation metrics for every model
## Training set
results_train %>%
  group_by(model) %>%
  summarise(RMSE = RMSE(.pred, truth),
            R_Square = R2_Score(.pred, truth),
            MAE = MAE(.pred, truth),
            MAPE = MAPE(.pred, truth))

## Test set
results_test %>%
  group_by(model) %>%
  summarise(RMSE = RMSE(.pred, truth),
            R_Square = R2_Score(.pred, truth),
            MAE = MAE(.pred, truth),
            MAPE = MAPE(.pred, truth))
```

#### Prediction vs Actual Comparison
```{r viz_comparison4, warning = FALSE, error = FALSE}
results_test %>%
  mutate(train = "testing") %>%
  bind_rows(results_train %>%
              mutate(train = "training")) %>%
  ggplot(aes(.pred, truth, color = model)) +
  geom_abline(lty = 2, size = 0.75, linetype = "dashed") +
  geom_point(alpha = 0.7) +
  facet_wrap(~train)
```

### Random Forest Model: Option 4.1 (All elements from ME-4ACD81)
Combination of Pb, Sc, Zn, Cd, Cu in predicting REE. Similar to the option 4, in this model, Tl is taken out considering its corr. coeff. is practically zero to REE. The idea is to combine variable with high importance but also have generally higher corr. coeff than the other element.

#### Data Preparation
```{r}

# Filter the training and test datasets to include predictors and target variables only
DT_train_fltr_alt_4_1 <- DT_train[,c("REE", "Pb", "Sc", "Zn", "Cd", "Cu")] 
DT_test_fltr_alt_4_1 <- DT_test[,c("REE","Pb", "Sc", "Zn", "Cd", "Cu")]

# Create a cross-validation table for resampling purposes during hyperparameter tuning
set.seed(2022)
DT_fltr_folds_4_1 <- vfold_cv(DT_train_fltr_alt_4_1, v = 5, repeats = 2)

```

#### Build the Model
```{r}
# Create a recipe for preprocessing
rf_recipe_4_1 <- recipe(REE ~ ., data = DT_train_fltr_alt_4_1)

# Create a workflow
rf_tuned_workflow_4_1 <- workflow() %>%
  add_recipe(rf_recipe_4_1) %>%
  add_model(rf_tuned_spec)

# Set-up the hyperparameter tuning
rf_tuned_grid_4_1 <- grid_regular(
  mtry(range = c(1,5)),
  min_n(range = c(1,20)),
  trees(range = c(500,2000)),
  levels = 5)

# Perform the hyperparameter tuning
set.seed(2022)
rf_random_tune_4_1 <- tune_grid(
  rf_tuned_workflow_4_1,
  resamples = DT_fltr_folds_4_1,
  grid = rf_tuned_grid_4_1,
  metrics = metric_set(rmse, rsq, mae)
)

# Show the best configurations
show_best(rf_random_tune_4_1, n = 3)

# Fit the best model into the training set
set.seed(2022)
rf_random_tuned_fit_4_1 <- finalize_workflow(rf_tuned_workflow_4_1, select_best(rf_random_tune_4_1)) %>%
    fit(DT_train_fltr_alt_4_1)

# Apply the trained recipe to the testing data
# 'bake()' will apply all recipe steps (e.g., imputation, normalization) to the testing dataset
DT_train_fltr_alt_4_1_prep <- bake(prep(rf_recipe_4_1), new_data = DT_train_fltr_alt_4_1)
DT_test_fltr_alt_4_1_prep <- bake(prep(rf_recipe_4_1), new_data = DT_test_fltr_alt_4_1)

# Make predictions on the preprocessed testing data
rf_tr_rdm_tuned_4_1_pred <- predict(rf_random_tuned_fit_4_1, new_data = DT_train_fltr_alt_4_1_prep)
rf_ts_rdm_tuned_4_1_pred <- predict(rf_random_tuned_fit_4_1, new_data = DT_test_fltr_alt_4_1_prep)
```

#### Evaluate the Model
```{r}
# RF Tuned Results
## Training set evaluation
results_train_rf_rdm_tuned_4_1 <- rf_tr_rdm_tuned_4_1_pred %>%
  bind_cols(DT_train_fltr_alt_4_1) %>%
  select(.pred, REE) %>%
  rename(truth = REE) %>%
  mutate(model = "RF_Rdm_Tuned_4_1")

## Test set evaluation
results_test_rf_rdm_tuned_4_1 <- rf_ts_rdm_tuned_4_1_pred %>%
  bind_cols(DT_test_fltr_alt_4_1) %>%
  rename(truth = REE) %>%
  mutate(model = "RF_Rdm_Tuned_4_1")

# Merge the results
## Training set
results_train <- bind_rows(results_train, results_train_rf_rdm_tuned_4_1)

## Test set
results_test <- bind_rows(results_test, results_test_rf_rdm_tuned_4_1)

# Show the evaluation metrics for every model
## Training set
results_train %>%
  group_by(model) %>%
  summarise(RMSE = RMSE(.pred, truth),
            R_Square = R2_Score(.pred, truth),
            MAE = MAE(.pred, truth),
            MAPE = MAPE(.pred, truth))

## Test set
results_test %>%
  group_by(model) %>%
  summarise(RMSE = RMSE(.pred, truth),
            R_Square = R2_Score(.pred, truth),
            MAE = MAE(.pred, truth),
            MAPE = MAPE(.pred, truth))
```

#### Prediction vs Actual Comparison
```{r viz_comparison4_1, warning = FALSE, error = FALSE}
results_test %>%
  mutate(train = "testing") %>%
  bind_rows(results_train %>%
              mutate(train = "training")) %>%
  ggplot(aes(truth, .pred, color = model)) +
  geom_abline(lty = 2, size = 0.75, linetype = "dashed") +
  geom_point(alpha = 0.7) +
  facet_wrap(~train)
```

### XGBoost Model: All elements from ME-4ACD81
Combination of All elements from ME-4ACD81 in predicting REE. This is the only model being performed with XGBoost considering the model of all elements from random forest produce the best performance.

#### Data Preparation
Data preparation is not needed for this model, because this model use the same data as what "Random Forest Option 3" use.

#### Build the Model
```{r}
# Build XGBoost Tuned Model
xgb_tuned_spec <- boost_tree(mode = "regression",
                             trees = 2000,
                             tree_depth = tune(),
                             learn_rate = tune(),
                             loss_reduction = tune(),
                             min_n = tune(),
                             sample_size = tune(),  
                             mtry = tune()) %>%
                 set_engine("xgboost")

# Define the recipe for XGBoost
xgb_recipe <- recipe(REE ~ ., data = DT_train_fltr_alt_3) 

# XGB tune workflow
xgb_tuned_workflow <- workflow() %>%
  add_model(xgb_tuned_spec) %>%
  add_recipe(xgb_recipe)


# Set-up the hyperparameter tuning
xgboost_params <- parameters(
  tree_depth(),
  learn_rate(),
  loss_reduction(),
  min_n(),
  sample_size = sample_prop(),
  finalize(mtry(), DT_train_fltr_alt_3))

xgboost_grid <- 
  dials::grid_space_filling(
    xgboost_params, 
    size = 20
  )


# Tune the model
set.seed(2022)
xgb_tune <- xgb_tuned_workflow %>% 
  tune_grid(resamples = DT_fltr_folds_3, 
            grid = xgboost_grid,
            metrics = metric_set(rmse, rsq, mae),
            control = control_grid(save_pred = TRUE))

# Show the best configurations
show_best(xgb_tune, n = 5, metric = "rmse")

# Fit the best model into the training set
set.seed(2022)
xgb_tuned_fit <- finalize_workflow(xgb_tuned_workflow, select_best(xgb_tune, metric = "rmse")) %>%
    fit(DT_train_fltr_alt_3)

# Make predictions on the test set
xgb_train_predictions <- predict(xgb_tuned_fit, new_data = DT_train_fltr_alt_3)

# Make predictions on the test set
xgb_test_predictions <- predict(xgb_tuned_fit, new_data = DT_test_fltr_alt_3)
```


#### Evaluate the Model
```{r}
# Evaluate performance on training data
xgb_train_results <- xgb_train_predictions %>%
  bind_cols(DT_train_fltr_alt_3) %>%
  select(.pred, REE) %>%
  rename(truth = REE) %>%
  mutate(model = "XGBoost_Tuned")

# Evaluate performance on test data
xgb_test_results <- xgb_test_predictions %>%
  bind_cols(DT_test_fltr_alt_3) %>%
  select(.pred, REE) %>%
  rename(truth = REE) %>%
  mutate(model = "XGBoost_Tuned")

# Merge the results
## Training set
results_train <- bind_rows(results_train, xgb_train_results)

## Test set
results_test <- bind_rows(results_test, xgb_test_results)


# Show the evaluation metrics for every model
## Training set
results_train %>%
  group_by(model) %>%
  summarise(RMSE = RMSE(.pred, truth),
            R_Square = R2_Score(.pred, truth),
            MAE = MAE(.pred, truth),
            MAPE = MAPE(.pred, truth))

## Test set
results_test %>%
  group_by(model) %>%
  summarise(RMSE = RMSE(.pred, truth),
            R_Square = R2_Score(.pred, truth),
            MAE = MAE(.pred, truth),
            MAPE = MAPE(.pred, truth))
```

#### Prediction vs Actual Comparison
```{r viz_comparison_xgb, warning = FALSE, error = FALSE}
results_test %>%
  mutate(train = "testing") %>%
  bind_rows(results_train %>%
              mutate(train = "training")) %>%
  ggplot(aes(.pred, truth, color = model)) +
  geom_abline(lty = 2, size = 0.75, linetype = "dashed") +
  geom_point(alpha = 0.7) +
  facet_wrap(~train)
```

### Selecting the best Model
#### Model Comparison
```{r}
# Display top 3 model
results_test %>%
  filter(model %in% c("RF_Rdm_Tuned_3", "RF_Default", "RF_Rdm_Tuned_4")) %>%
  group_by(model) %>%
  summarise(RMSE = RMSE(.pred, truth),
            R_Square = R2_Score(.pred, truth),
            MAE = MAE(.pred, truth),
            MAPE = MAPE(.pred, truth))
```

Random Forest Model 3 is the best model, beating every model on RMSE, R-square, and MAPE.

#### Prediction vs Actual Comparison of Best Model
```{r pred_res, fig.cap = "Actual vs Prediction Results", fig.width = 8, fig.height = 8, fig.align = "center", warning = FALSE, error = FALSE}
# results_test_best <- results_test %>%
#   filter(model %in% c("RF_Rdm_Tuned_3")) %>%
#   mutate(train = "Testing") %>%
#   bind_rows((results_train %>% filter(model %in% c("RF_Rdm_Tuned_3"))) %>%
#               mutate(train = "Training")) 

results_test_best <- results_test %>%
  filter(model %in% c("RF_Rdm_Tuned_3")) %>%
  ggplot(aes(.pred, truth)) +
  geom_abline(lty = 2, linewidth = 0.75, linetype = "dashed", colour='red') +
  geom_point(alpha = 0.7, colour = "black", size = 5) +
  xlim(c(0,400)) +
  ylim(c(0,400)) +
  xlab("Predicted") +
  ylab("Actual") +
  # facet_wrap(~train) +
  theme_minimal() +
  theme(axis.title.x = element_text(size = 20),            # Increase X-axis title size
        axis.title.y = element_text(size = 20))            # Increase Y-axis title size

print(results_test_best)
```


## Predictive Modelling (HREE)

