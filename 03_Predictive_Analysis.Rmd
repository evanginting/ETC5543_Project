---
title: "03_Predictive_Analysis"
output: html_document
date: "2024-10-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidymodels)
library(ranger)
library(randomForestSRC)
library(randomForest)
library(data.table)
library(xgboost)
library(vip)
```


## Prepare data for modelling

Note this is where you would remove outliers from your data set (if you have good reason to remove)

```{r}
#Load data
alldata <- readRDS('./results/all_data.rds')

DT <- setDT(alldata)

#reformat data into wide format
DT <- dcast(DT,Project_Name+Sample_ID~Element_Symbol, value.var = c("Element_Value_ppm"), fun=mean, fill=NA)

```

Split the train test data evenly among projects (stratify by project). Then balance the training sample data using over/oversampling so that each project area has the same number of samples. 


```{r}
library(scutr)

scut_DT <- DT[,c("Project_Name", "REE","Li","Mo","Cu")]
scut_DT <- na.omit(scut_DT)

set.seed(4)
#split the data into test and train stratified by project (to make sure all projects are represented in the train/test data set)
scut_DT_intital <- initial_split(scut_DT, prop = 0.75, strata = "Project_Name") 
# Create data frames for the two sets:
scut_DT_train <- training(scut_DT_intital)
scut_DT_test  <- testing(scut_DT_intital)

#now apply over/undersampling on the training data to make sure each project has the same number of samples to avoid sampling bias
scut_DT_train  <- SCUT(scut_DT_train , "Project_Name", oversample = resample_random, undersample = resample_random)

scut_DT_train [, .(count = .N), by = Project_Name] #here you can see that each project has been randomly over/undersampled so that there are 13 samples in each project - this removes class imbalance


```


## Split data into train and test data sets

Note this is a very simple split across all data sets - you may need to split by project as well to improve model performance

```{r}
set.seed(2022)

DT_initial <- initial_split(DT, prop = 0.75)
DT_train <- training(DT_initial)
DT_test <- testing(DT_initial)

```


## Very basic predictive modelling

Predict REE element concentration from some of the elements in the ME-4ACD81 test (this is the cheaper test). Elements in this test include Ag, As, Cd, Co, Cu, Li, Mo, Ni, Pb, Sc, Tl, Zn. 

Tl is not well represented, lets try and use Li, Mo and Cu (take alook at your pearsons correlation diagrams - there may be some associations)

### First Try
```{r}

#make sure that no NA result exist amongst the data set
rf_train <- DT_train[,c("REE","Li","Mo","Cu")]
rf_test <- DT_test[,c("REE","Li","Mo","Cu")]



#tune the model on training data

rf_tune <- rfsrc(REE~.,data=rf_train,ntreeTry = 500)


#fit the optimised model to the training data:

rf_fit <- rfsrc(REE~.,data=rf_train, mtry = rf_tune$optimal[2], nodesize = rf_tune$optimal[1], ntree = 500)

#predict on test data:
rf_pred <- predict(rf_fit, rf_test)


```


### Alternative
#### Build the model: Option 1 (Li, Mo, Cu)

Combination of Li, Mo, Cu in predicting REE
```{r}

#make sure that no NA result exist amongst the data set
DT_train_fltr_alt <- DT_train[,c("REE","Li","Mo","Cu")] %>% na.omit()
DT_test_fltr_alt <- DT_test[,c("REE","Li","Mo","Cu")] %>% na.omit()
DT_fltr_folds <- vfold_cv(DT_train_fltr_alt, v = 5, repeats = 2)

# Build RF Model
rf_spec <- rand_forest(mode = "regression") %>%
  set_engine("randomForest")

rf_fit <- rf_spec %>%
  fit(REE ~ ., data = DT_train_fltr_alt)

rf_fit
```

```{r}
# Build RF Tuned Model
rf_tuned_spec <- rand_forest(mode = "regression",
                       mtry = tune(),
                       min_n = tune(),
                       trees = tune()) %>%
                 set_engine("randomForest")

# Create a recipe for preprocessing
rf_recipe <- recipe(REE ~ Li + Mo + Cu, data = DT_train_fltr_alt) %>%
  step_normalize(all_predictors())

rf_tuned_workflow <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_tuned_spec)

rf_tuned_grid <- grid_regular(
  mtry(range = c(1,3)),
  min_n(range = c(5,15)),
  trees(range = c(500,1000)),
  levels = 5)

set.seed(2022)
rf_random_tune <- tune_grid(
  rf_tuned_workflow,
  resamples = DT_fltr_folds,
  grid = rf_tuned_grid,
  metrics = metric_set(rmse, rsq, mae)
)

show_best(rf_random_tune, n = 3)

rf_random_tuned_fit <- finalize_workflow(rf_tuned_workflow, select_best(rf_random_tune)) %>%
    fit(DT_train_fltr_alt)
```


```{r}
# Build LM Model
lm_spec <- linear_reg() %>%
  set_engine("lm")

lm_fit <- lm_spec %>%
  fit(REE ~ ., data = DT_train_fltr_alt)

lm_fit
```

```{r}
# Build XGBoost Tuned Model
xgb_tuned_spec <- boost_tree(mode = "regression",
                             trees = 1000,
                             tree_depth = tune(),
                             learn_rate = tune(),
                             loss_reduction = tune(),
                             min_n = tune()) %>%
                 set_engine("xgboost")

# xgb tune workflow
xgb_tuned_workflow <- workflow() %>%
  add_model(xgb_tuned_spec) %>%
  add_formula(REE ~ .)


# grid specification
xgboost_params <- parameters(
    tree_depth(),
    learn_rate(),
    loss_reduction(),
    min_n()
  )

xgboost_grid <- 
  dials::grid_space_filling(
    xgboost_params, 
    size = 10
  )

head(xgboost_grid)


# Tune the model
set.seed(2022)
xgb_tune <- xgb_tuned_workflow %>% 
  tune_grid(resamples = DT_fltr_folds, 
            grid = xgboost_grid)

show_best(xgb_tune, n = 3)

xgb_tuned_fit <- finalize_workflow(xgb_tuned_workflow, select_best(xgb_tune)) %>%
    fit(DT_train_fltr_alt)
```

#### Evaluate the model
```{r}

# RF Results
results_train_rf <- rf_fit %>%
  predict(new_data = DT_train_fltr_alt) %>%
  mutate(truth = DT_train_fltr_alt$REE,
         model = "RF")

results_test_rf <- rf_fit %>%
  predict(new_data = DT_test_fltr_alt) %>%
  mutate(truth = DT_test_fltr_alt$REE,
         model = "RF")

# RF Tuned Results
results_train_rf_rdm_tuned <- rf_random_tuned_fit %>%
  predict(new_data = DT_train_fltr_alt) %>%
  mutate(truth = DT_train_fltr_alt$REE,
         model = "RF_Rdm_Tuned")

results_test_rf_rdm_tuned <- rf_random_tuned_fit %>%
  predict(new_data = DT_test_fltr_alt) %>%
  mutate(truth = DT_test_fltr_alt$REE,
         model = "RF_Rdm_Tuned")

# LM Results
results_train_lm <- lm_fit %>%
  predict(new_data = DT_train_fltr_alt) %>%
  mutate(truth = DT_train_fltr_alt$REE,
         model = "LM")

results_test_lm <- lm_fit %>%
  predict(new_data = DT_test_fltr_alt) %>%
  mutate(truth = DT_test_fltr_alt$REE,
         model = "LM")

# XGB Tuned Results
results_train_xgb_tuned <- xgb_tuned_fit %>%
  predict(new_data = DT_train_fltr_alt) %>%
  mutate(truth = DT_train_fltr_alt$REE,
         model = "XGB_Tuned")

results_test_xgb_tuned <- xgb_tuned_fit %>%
  predict(new_data = DT_test_fltr_alt) %>%
  mutate(truth = DT_test_fltr_alt$REE,
         model = "XGB_Tuned")

# Merge the results
results_train <- bind_rows(results_train_rf, results_train_rf_rdm_tuned, results_train_lm, results_train_xgb_tuned)

results_test <- bind_rows(results_test_rf, results_test_rf_rdm_tuned, results_test_lm, results_test_xgb_tuned)
```


```{r}
results_train %>%
  group_by(model) %>%
  metrics(truth = truth, estimate = .pred)

results_test %>%
  group_by(model) %>%
  metrics(truth = truth, estimate = .pred)
```

```{r}
results_test %>%
  mutate(train = "testing") %>%
  bind_rows(results_train %>%
              mutate(train = "training")) %>%
  ggplot(aes(truth, .pred, color = model)) +
  geom_abline(lty = 2, size = 0.75, linetype = "dashed") +
  geom_point(alpha = 0.7) +
  facet_wrap(~train)
```

#### Build the model: Option 2 (Pb, Sc, Zn)
Combination of Pb, Sc, Zn in predicting REE. These three elements have quiet high correlation with REE where their correlation coeff. are above 0,5.

```{r}

#make sure that no NA result exist amongst the data set
DT_train_fltr_alt_2 <- DT_train[,c("REE","Pb","Sc")] %>% na.omit()
DT_test_fltr_alt_2 <- DT_test[,c("REE","Pb","Sc")] %>% na.omit()
DT_fltr_folds_2 <- vfold_cv(DT_train_fltr_alt_2, v = 5, repeats = 2)

```

```{r}


# Create a recipe for preprocessing
rf_recipe_2 <- recipe(REE ~ ., data = DT_train_fltr_alt_2) %>%
  step_normalize(all_predictors())

rf_tuned_workflow_2 <- workflow() %>%
  add_recipe(rf_recipe_2) %>%
  add_model(rf_tuned_spec)

rf_tuned_grid_2 <- grid_regular(
  mtry(range = c(1,2)),
  min_n(range = c(1,15)),
  trees(range = c(500,2000)),
  levels = 5)

set.seed(2022)
rf_random_tune_2 <- tune_grid(
  rf_tuned_workflow_2,
  resamples = DT_fltr_folds_2,
  grid = rf_tuned_grid_2,
  metrics = metric_set(rmse, rsq, mae)
)

show_best(rf_random_tune_2, n = 3)

rf_random_tuned_fit_2 <- finalize_workflow(rf_tuned_workflow_2, select_best(rf_random_tune_2)) %>%
    fit(DT_train_fltr_alt_2)
```

#### Evaluate the model and compare to another combination
```{r}
# RF Tuned Results 2
results_train_rf_rdm_tuned_2 <- rf_random_tuned_fit_2 %>%
  predict(new_data = DT_train_fltr_alt_2) %>%
  mutate(truth = DT_train_fltr_alt_2$REE,
         model = "RF_Rdm_Tuned_2")

results_test_rf_rdm_tuned_2 <- rf_random_tuned_fit_2 %>%
  predict(new_data = DT_test_fltr_alt_2) %>%
  mutate(truth = DT_test_fltr_alt_2$REE,
         model = "RF_Rdm_Tuned_2")

# Merge the results
results_train <- bind_rows(results_train, results_train_rf_rdm_tuned_2)

results_test <- bind_rows(results_test, results_test_rf_rdm_tuned_2)
```

```{r}
results_train %>%
  group_by(model) %>%
  metrics(truth = truth, estimate = .pred)

results_test %>%
  group_by(model) %>%
  metrics(truth = truth, estimate = .pred)
```

```{r}
results_test %>%
  mutate(train = "testing") %>%
  bind_rows(results_train %>%
              mutate(train = "training")) %>%
  ggplot(aes(truth, .pred, color = model)) +
  geom_abline(lty = 2, size = 0.75, linetype = "dashed") +
  geom_point(alpha = 0.7) +
  facet_wrap(~train)
```


#### Build the model: Option 3 (All elements from ME-4ACD81 except As)
Combination of All elements from ME-4ACD81 except As in predicting REE.

```{r}

#make sure that no NA result exist amongst the data set
DT_train_fltr_alt_3 <- DT_train[,c("REE","Ag", "Cd", "Co", "Cu", "Li", "Mo", "Ni", "Pb", "Sc", "Tl", "Zn")] %>% na.omit()
DT_test_fltr_alt_3 <- DT_test[,c("REE","Ag", "Cd", "Co", "Cu", "Li", "Mo", "Ni", "Pb", "Sc", "Tl", "Zn")] %>% na.omit()
DT_fltr_folds_3 <- vfold_cv(DT_train_fltr_alt_3, v = 5, repeats = 2)

```

```{r}
# Create a recipe for preprocessing
rf_recipe_3 <- recipe(REE ~ ., data = DT_train_fltr_alt_3) %>%
  step_normalize(all_predictors())

rf_tuned_workflow_3 <- workflow() %>%
  add_recipe(rf_recipe_3) %>%
  add_model(rf_tuned_spec)

rf_tuned_grid_3 <- grid_regular(
  mtry(range = c(1,11)),
  min_n(range = c(1,12)),
  trees(range = c(500,1000)),
  levels = 5)

set.seed(2022)
rf_random_tune_3 <- tune_grid(
  rf_tuned_workflow_3,
  resamples = DT_fltr_folds_3,
  grid = rf_tuned_grid_3,
  metrics = metric_set(rmse, rsq, mae)
)


show_best(rf_random_tune_3, n = 3)

rf_random_tuned_fit_3 <- finalize_workflow(rf_tuned_workflow_3, select_best(rf_random_tune_3)) %>%
    fit(DT_train_fltr_alt_3)
```

```{r}
# Plot variable importance (optional)
rf_random_tuned_fit_3 %>%
  extract_fit_parsnip() %>%
  vip::vip()
```


#### Evaluate the model and compare to another combination
```{r}
# RF Tuned Results 2
results_train_rf_rdm_tuned_3 <- rf_random_tuned_fit_3 %>%
  predict(new_data = DT_train_fltr_alt_3) %>%
  mutate(truth = DT_train_fltr_alt_3$REE,
         model = "RF_Rdm_Tuned_3")

results_test_rf_rdm_tuned_3 <- rf_random_tuned_fit_3 %>%
  predict(new_data = DT_test_fltr_alt_3) %>%
  mutate(truth = DT_test_fltr_alt_3$REE,
         model = "RF_Rdm_Tuned_3")

# Merge the results
results_train <- bind_rows(results_train, results_train_rf_rdm_tuned_3)

results_test <- bind_rows(results_test, results_test_rf_rdm_tuned_3)
```

```{r}
results_train %>%
  group_by(model) %>%
  metrics(truth = truth, estimate = .pred)

results_test %>%
  group_by(model) %>%
  metrics(truth = truth, estimate = .pred)
```


```{r}
results_test %>%
  mutate(train = "testing") %>%
  bind_rows(results_train %>%
              mutate(train = "training")) %>%
  ggplot(aes(truth, .pred, color = model)) +
  geom_abline(lty = 2, size = 0.75, linetype = "dashed") +
  geom_point(alpha = 0.7) +
  facet_wrap(~train)
```

#### Build the model: Option 4 (Sc, Cd, Pb, Tl)
Combination of Sc, Cd, Pb, Tl in predicting REE. These four elements have high variable importance when creating the model with all elements included.

```{r}

#make sure that no NA result exist amongst the data set
DT_train_fltr_alt_4 <- DT_train[,c("REE","Sc","Cd","Pb", "Tl")] %>% na.omit()
DT_test_fltr_alt_4 <- DT_test[,c("REE","Sc","Cd","Pb", "Tl")] %>% na.omit()
DT_fltr_folds_4 <- vfold_cv(DT_train_fltr_alt_4, v = 5, repeats = 2)

```

```{r}


# Create a recipe for preprocessing
rf_recipe_4 <- recipe(REE ~ ., data = DT_train_fltr_alt_4) %>%
  step_normalize(all_predictors())

rf_tuned_workflow_4 <- workflow() %>%
  add_recipe(rf_recipe_4) %>%
  add_model(rf_tuned_spec)

rf_tuned_grid_4 <- grid_regular(
  mtry(range = c(1,4)),
  min_n(range = c(1,15)),
  trees(range = c(500,2000)),
  levels = 5)

set.seed(2022)
rf_random_tune_4 <- tune_grid(
  rf_tuned_workflow_4,
  resamples = DT_fltr_folds_4,
  grid = rf_tuned_grid_4,
  metrics = metric_set(rmse, rsq, mae)
)

show_best(rf_random_tune_4, n = 3)

rf_random_tuned_fit_4 <- finalize_workflow(rf_tuned_workflow_4, select_best(rf_random_tune_4)) %>%
    fit(DT_train_fltr_alt_4)
```

#### Evaluate the model and compare to another combination
```{r}
# RF Tuned Results 2
results_train_rf_rdm_tuned_4 <- rf_random_tuned_fit_4 %>%
  predict(new_data = DT_train_fltr_alt_4) %>%
  mutate(truth = DT_train_fltr_alt_4$REE,
         model = "RF_Rdm_Tuned_4")

results_test_rf_rdm_tuned_4 <- rf_random_tuned_fit_4 %>%
  predict(new_data = DT_test_fltr_alt_4) %>%
  mutate(truth = DT_test_fltr_alt_4$REE,
         model = "RF_Rdm_Tuned_4")

# Merge the results
results_train <- bind_rows(results_train, results_train_rf_rdm_tuned_4)

results_test <- bind_rows(results_test, results_test_rf_rdm_tuned_4)
```

```{r}
results_train %>%
  group_by(model) %>%
  metrics(truth = truth, estimate = .pred)

results_test %>%
  group_by(model) %>%
  metrics(truth = truth, estimate = .pred)
```

```{r}
results_test %>%
  mutate(train = "testing") %>%
  bind_rows(results_train %>%
              mutate(train = "training")) %>%
  ggplot(aes(truth, .pred, color = model)) +
  geom_abline(lty = 2, size = 0.75, linetype = "dashed") +
  geom_point(alpha = 0.7) +
  facet_wrap(~train)
```


```{r}
results_train %>%
  group_by(model) %>%
  filter(model %in% c("RF_Rdm_Tuned_2","RF_Rdm_Tuned_4")) %>%
  metrics(truth = truth, estimate = .pred)

results_test %>%
  group_by(model) %>%
  filter(model %in% c("RF_Rdm_Tuned_2","RF_Rdm_Tuned_4")) %>%
  metrics(truth = truth, estimate = .pred)
```



------------INCLUDING PROJECT NAME---------------------------

```{r}
library(scutr)

scut_DT_2 <- DT[,c("Project_Name", "REE","Pb","Sc")]
scut_DT_2 <- na.omit(scut_DT_2)

set.seed(4)
#split the data into test and train stratified by project (to make sure all projects are represented in the train/test data set)
scut_DT_initial_2 <- initial_split(scut_DT_2, prop = 0.75, strata = "Project_Name") 
# Create data frames for the two sets:
scut_DT_train_2 <- training(scut_DT_initial_2)
scut_DT_test_2  <- testing(scut_DT_initial_2)

#now apply over/undersampling on the training data to make sure each project has the same number of samples to avoid sampling bias
scut_DT_train_2  <- SCUT(scut_DT_train_2, "Project_Name", oversample = resample_random, undersample = resample_random)

scut_DT_train_2 [, .(count = .N), by = Project_Name] #here you can see that each project has been randomly over/undersampled so that there are 12 samples in each project - this removes class imbalance


scut_DT_train_2_folds <- vfold_cv(scut_DT_train_2, v = 5, repeats = 2)
```

```{r}

# Build RF Tuned Model
## rf_tuned_spec <- rand_forest(mode = "regression",
##                        mtry = tune(),
##                        min_n = tune(),
##                        trees = tune()) %>%
##                  set_engine("randomForest")
## 
## rf_tuned_workflow <- workflow() %>%
##   add_variables(outcomes = REE, predictors = everything()) %>%
##   add_model(rf_tuned_spec)

rf_tuned_recipe_5 <- recipe(REE ~ Project_Name + Pb + Sc, data = scut_DT_train_2) %>%
  step_novel(Project_Name) %>%            # Handle new/unseen categories in Project_Name
  step_dummy(all_nominal(), -all_outcomes()) %>% # One-hot encoding for Project_Name
  step_normalize(all_predictors())

rf_tuned_workflow_5 <- workflow() %>%
  add_recipe(rf_tuned_recipe_5) %>%
  add_model(rf_tuned_spec)

rf_tuned_grid_5 <- grid_regular(
  mtry(range = c(1,2)),
  min_n(range = c(1,15)),
  trees(range = c(500,2000)),
  levels = 5)


set.seed(2022)
rf_random_tune_5 <- tune_grid(
  rf_tuned_workflow_5,
  resamples = scut_DT_train_2_folds,
  grid = rf_tuned_grid_5,
  metrics = metric_set(rmse, rsq, mae)
)


show_best(rf_random_tune_5, n = 3)

rf_random_tuned_fit_5 <- finalize_workflow(rf_tuned_workflow_5, select_best(rf_random_tune_5)) %>%
    fit(scut_DT_train_2)
```


#### Evaluate the model and compare to another combination (For each project)
```{r}
# RF Tuned Results 2 (Project)
results_train_rf_rdm_tuned_5 <- rf_random_tuned_fit_5 %>%
  predict(new_data = scut_DT_train_2) %>%
  mutate(Project_Name = scut_DT_train_2$Project_Name,
         truth = scut_DT_train_2$REE,
         model = "Proj_RF_Mnl_Tuned_2")

results_test_rf_rdm_tuned_5 <- rf_random_tuned_fit_5 %>%
  predict(new_data = scut_DT_test_2) %>%
  mutate(Project_Name = scut_DT_test_2$Project_Name,
         truth = scut_DT_test_2$REE,
         model = "Proj_RF_Mnl_Tuned_2")

# Merge the results
results_train_proj <- results_train_rf_rdm_tuned_5

results_test_proj <- results_test_rf_rdm_tuned_5
```

```{r}
results_train_proj %>%
  group_by(Project_Name) %>%
  metrics(truth = truth, estimate = .pred)

results_test_proj %>%
  group_by(Project_Name) %>%
  metrics(truth = truth, estimate = .pred)
```

```{r}
results_test_proj %>%
  mutate(train = "testing") %>%
  bind_rows(results_train_proj %>%
              mutate(train = "training")) %>%
  ggplot(aes(truth, .pred, color = Project_Name)) +
  geom_abline(lty = 2, size = 0.75, linetype = "dashed") +
  geom_point(alpha = 0.7) +
  facet_wrap(~train)
```